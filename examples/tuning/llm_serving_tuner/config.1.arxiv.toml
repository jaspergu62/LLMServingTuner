# -------------------------寻优相关参数配置------------------------------------------------
n_particles = 10
iters = 5
ttft_penalty = 1
tpot_penalty = 0
success_rate_penalty = 5.0
ttft_slo = 1
tpot_slo = 0.05

# 自定义终止寻优条件，当相对上一轮的相对误差小于这个值时，考虑终止寻优, -inf表示，不自动终止，跑完所有迭代计划。
ftol = -inf
ftol_iter = 3 # 自定义终止条件，第几次满足相对误差的时候，进行终止。
sample_size = 1000 # 对原始数据集采样大小，用采样后的数据进行调优。
mem_coefficient = 0.8 # kv 显示使用剩余显存系数，
scaling_coefficient = 1.3 # 理论推导计算出max batch size上限后，将上限缩放的比例
theory_guided_enable = true # 是否开启理论推导，来压缩部分搜索空间,使用前 请确保显存未被占用，否则会无效。

# service = "master" # 服务是双机运行时需要使用。 主节点服务配置为master，从节点服务配置为slave。

[pso_options]
# 寻优算法 寻优方向和速度控制参数
c1 = 2.0 # 推荐范围 0-4, c1 c2 2, c1 1.6和c2 1.8, c1 1.6 和c2 2
c2 = 2.0
w = 1.8 # 推荐范围0.4,2， 典型取值，0.9  1.2 1.5  1.8
[pso_strategy]
# 寻优算法，各个寻优控制参数的变化策略，支持 exp_decay, nonlin_mod, lin_variation, random
w = "exp_decay"
c1 = "exp_decay"
c2 = "exp_decay"

# -------------------------CBO代理模型配置 (默认开启)------------------------------------------------
[cbo]
enabled = true              # 是否启用CBO代理模型 (设置为false可关闭)
initial_points = 5          # 启用代理模型前的真实评估次数
strategy = "conservative"   # 候选筛选策略
feasibility_threshold = -1.2  # 可行性阈值

# -------------------------选取top_k进入精调------------------------------------------------
[data_storage]
pso_top_k = 0


# -------------------------测评工具相关配置------------------------------------------------
[vllm_benchmark.command]
host = "127.0.0.1"
port = "8000"
model = "/cache/pretrained_models/Llama-3.1-8B-Instruct/"
served_model_name = "Llama-3.1-8B-Instruct"
dataset_name = "vidur"
num_prompts = 1000
others = "--tokenizer /cache/pretrained_models/Llama-3.1-8B-Instruct/ --dataset-path /cache/simserving/LLMServingTuner-merge3/data/Arxiv-Summarization/arxiv_summarization_prefill_decode.csv --vidur-prefill-scale 1.0 --vidur-decode-scale 1.0 --burstiness 0.7 --seed 42"



# -------------------------vllm相关配置------------------------------------------------
[vllm]
[vllm.command]
host = "127.0.0.1"
port = "8010"
model = "/cache/pretrained_models/Llama-3.1-8B-Instruct/"
served_model_name = "Llama-3.1-8B-Instruct"
others = ""
[[vllm.target_field]]
name = "MAX_NUM_BATCHED_TOKENS"
config_position = "env"
min = 8192
max = 65536
dtype = "int"
value = 8192
[[vllm.target_field]]
name = "MAX_NUM_SEQS"
config_position = "env"
min = 32
max = 512
dtype = "int"
value = 64
[[vllm.target_field]]
name = "CONCURRENCY" 
config_position = "env"
min = 1
max = 1000
dtype = "int"
value = 100
[[vllm.target_field]]
name = "REQUESTRATE" 
config_position = "env"
min = 2.3
max = 2.3
dtype = "float"
value = 2.3

# -------------------------mindie相关配置------------------------------------------------
[mindie]
[[mindie.target_field]]
name = "max_batch_size"
config_position = "BackendConfig.ScheduleConfig.maxBatchSize"
min = 10
max = 1000
dtype = "int"
[[mindie.target_field]]
name = "max_prefill_batch_size"
config_position = "BackendConfig.ScheduleConfig.maxPrefillBatchSize"  # 该值不允许大于maxBatchSize
min = 0.1
max = 0.7
dtype = "ratio"
dtype_param = "max_batch_size"
[[mindie.target_field]]
name = "prefill_time_ms_per_req"
config_position = "BackendConfig.ScheduleConfig.prefillTimeMsPerReq"
min = 0
max = 1000
dtype = "range"
dtype_param = 10
[[mindie.target_field]]
name = "decode_time_ms_per_req"
config_position = "BackendConfig.ScheduleConfig.decodeTimeMsPerReq"
min = 0
max = 1000
dtype = "range"
dtype_param = 10
[[mindie.target_field]]
name = "support_select_batch"
config_position = "BackendConfig.ScheduleConfig.supportSelectBatch"
min = 0
max = 1
dtype = "bool"
[[mindie.target_field]]
name = "max_queue_deloy_mircroseconds"
config_position = "BackendConfig.ScheduleConfig.maxQueueDelayMicroseconds"
min = 500
max = 1000000
dtype = "range"
dtype_param = 100
[[mindie.target_field]]
name = "max_preempt_count"
config_position = "BackendConfig.ScheduleConfig.maxPreemptCount"
min = 0
max = 1
dtype = "ratio"
dtype_param = "max_batch_size"
[[mindie.target_field]]
name = "CONCURRENCY"  # 支持范围0-1000
config_position = "env"
min = 1
max = 1001
dtype = "int"
value = 100
[[mindie.target_field]]
name = "REQUESTRATE" # 支持范围0-10000
config_position = "env"
min = 1
max = 1001
dtype = "float"
value = 100


# ----------------------latency model相关配置------------------------------------------------
[latency_model]
predictor_type = "xgboost"

# XGBoost configuration (used when predictor_type = "xgboost")
model_path = "/tmp/latency_model/model/xgb_model.ubj"

# Vidur configuration (used when predictor_type = "vidur")
# predictor_type = "vidur"
# vidur_model_name = "meta-llama/Llama-2-7b-hf"
# vidur_device = "a100"
# vidur_network_device = "a100_pairwise_nvlink"
# vidur_tensor_parallel_size = 1
# vidur_num_pipeline_stages = 1
# vidur_block_size = 16
# vidur_predictor_type = "random_forest"  # or "linear_regression"
# vidur_cache_dir = "cache"
# vidur_prediction_max_batch_size = 128
# vidur_prediction_max_tokens_per_request = 4096
# Optional: Override profiling data paths
# vidur_compute_input_file = "./data/profiling/compute/a100/llama-2-7b/mlp.csv"
# vidur_attention_input_file = "./data/profiling/compute/a100/llama-2-7b/attention.csv"

# -------------------------kubectl相关配置------------------------------------------------
[kubectl]
kubectl_default_path = ""
